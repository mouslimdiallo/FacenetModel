{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mouslimdiallo/FacenetModel/blob/main/FaceNet_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nrl1g3a6fyGx",
        "outputId": "91f0f133-9f3a-4af8-a191-7a2bd8f037a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import files\n",
        "#files.upload()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWyPHA1W8PX-",
        "outputId": "4554d554-36f8-4a33-a577-473d39a09000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'FaceNet_Recognition'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "Cloning into 'keras-facenet'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Total 57 (delta 0), reused 0 (delta 0), pack-reused 57\u001b[K\n",
            "Unpacking objects: 100% (57/57), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mouslimdiallo/FaceNet_Recognition.git\n",
        "!git clone https://github.com/nyoki-mtl/keras-facenet.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf98FIkaOvMc",
        "outputId": "6a718554-59b1-4048-b88c-6cbf7e498cae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Colab Notebooks/Reconnaissance Faciale Project/keras-facenet.zip\n",
            "  inflating: keras-facenet/model/facenet_keras.h5  \n",
            "  inflating: keras-facenet/weights/facenet_keras_weights.h5  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/Colab\\ Notebooks/Reconnaissance\\ Faciale\\ Project/keras-facenet.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SkCj8NSPLUxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ddaced5-ad7d-4c8d-e9a8-8f6b08094fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "[<KerasTensor: shape=(None, 160, 160, 3) dtype=float32 (created by layer 'input_1')>]\n",
            "[<KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'Bottleneck_BatchNorm')>]\n"
          ]
        }
      ],
      "source": [
        "# import du modele FaceNet Keras\n",
        "from keras.models import load_model\n",
        "# Charger le modele\n",
        "model = load_model('/content/keras-facenet/model/facenet_keras.h5')\n",
        "# Resumer des input et output\n",
        "print(model.inputs)\n",
        "print(model.outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94JMKWP7QOfS"
      },
      "source": [
        "Là nous avons un modèle keras-facenet prêt à l'emploi avec des tenseurs d'entrée et de sorties qui fonctionnent avec des images pixelisées 160x160 px en entrée."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSernZc1Ra9u"
      },
      "source": [
        "Installation de MTCNN, PIL, PLOT et Numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lehHloTQKrCF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e586b6-0582-4a64-a5bc-6f5e24913751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mtcnn\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (2.7.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.19.5)\n",
            "Installing collected packages: mtcnn\n",
            "Successfully installed mtcnn-0.1.1\n",
            "Collecting praw\n",
            "  Downloading praw-7.5.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting prawcore<3,>=2.1\n",
            "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
            "Collecting websocket-client>=0.54.0\n",
            "  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.5 MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.18\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2.1->praw) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\n",
            "Installing collected packages: websocket-client, update-checker, prawcore, praw\n",
            "Successfully installed praw-7.5.0 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.2.3\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "# installation et import des bibliothèques necessaires\n",
        "!pip install mtcnn\n",
        "!pip install praw\n",
        "!pip install Pillow\n",
        "!pip install numpy\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xy-md7oXlRSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2cce1b1-0198-4a67-a92e-2e76f8c626aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.10.8)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "# Pour un dataset sur Kaggle, voici comment l'importer directement sur Colab.\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "txHN0a9Im4Zf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b433113-cbcb-4497-aa59-90e1a6dc4550"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 5-celebrity-faces-dataset.zip to /content\n",
            "\r  0% 0.00/5.03M [00:00<?, ?B/s]\n",
            "\r100% 5.03M/5.03M [00:00<00:00, 69.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# La commande pour telecharger notre dataset kaggle\n",
        "!kaggle datasets download -d dansbecker/5-celebrity-faces-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoGNn3sFpIt1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ec1093-1124-4614-8160-6c6f0fdae343"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/5-celebrity-faces-dataset.zip\n",
            "replace data/train/ben_afflek/httpcsvkmeuaeccjpg.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "# dezipper les dataset kaggle des celebrités\n",
        "!unzip /content/5-celebrity-faces-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQoagCAPyix9"
      },
      "source": [
        "Import d'un dataset kaggle pour la classification d'images de celebrités : train et test/\n",
        "\n",
        "On fait l'entrainemnt dans train, et nous classons dans validation. Exemple à reproduire avec d'autres dataset.\n",
        "\n",
        "detectons chaque visage dans chaque photo (celebrité ) et faire une extraction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRtHMOy2fK2y"
      },
      "source": [
        "L'objectif est de prédire l'identité d'un visage donné avec un classificateur SVM (Linear Support Vector Machine)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "St7bpIG9K6A7"
      },
      "outputs": [],
      "source": [
        "# import des outils face detection avec mtcnn\n",
        "from os import listdir\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "from matplotlib import pyplot\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Extraction d'un seul visage avec px 160\n",
        "def extract_face(filename, required_size=(160, 160)):\n",
        "  # charger l'image à partir du fichier\n",
        "  image = Image.open(filename)\n",
        "  # convertir en l'image en RGB\n",
        "  image = image.convert('RGB')\n",
        "  # convertir en tableau\n",
        "  pixels = asarray(image)\n",
        "  # creer notre detecteur de visage mtcnn\n",
        "  detector = MTCNN()\n",
        "  # detection du visage stocké dans results\n",
        "  results = detector.detect_faces(pixels)\n",
        "  # extraire le cadre de limitation de la première face\n",
        "  x1, y1, width, height = results[0]['box']\n",
        "\n",
        "  # corriger les erreurs\n",
        "  x1, y1 = abs(x1), abs(y1)\n",
        "  x2, y2 = x1 + width, y1 + height\n",
        "  # extraire le visage\n",
        "  face = pixels[y1:y2, x1:x2]\n",
        "  # redimension des pixels du modele\n",
        "  image = Image.fromarray(face)\n",
        "  image = image.resize(required_size)\n",
        "  face_array = asarray(image)\n",
        "  return face_array\n",
        "\n",
        "# spécificions le dossier dans lequel on va faire le traçage\n",
        "folder = '/content/data/train/ben_afflek/'\n",
        "i = 1\n",
        "# Faire une liste d'énumeration des fichiers dans le dossier\n",
        "for filename in listdir(folder):\n",
        "# Le chemin d'accès\n",
        "\tpath = folder + filename\n",
        "\t  # Obtention du visage\n",
        "\tface = extract_face(path)\n",
        "\tprint(i, face.shape)\n",
        "\t# faire la tracée\n",
        "\tpyplot.subplot(2, 7, i)\n",
        "\tpyplot.axis('off')\n",
        "\tpyplot.imshow(face)\n",
        "\ti += 1\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YDp9jzU4HE1"
      },
      "source": [
        "Chargement de chaque visage dans un tableau numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h0ihZ7SllXh"
      },
      "source": [
        "Voici le chargement de chaque photo sous forme de tableau numpy. Mainteannt parcourons chaque sous repertoire afin de detecter les visages pour coller des etiquettes dessus. Utilisons des fonctions pour cela."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TFzkN_ms0kt"
      },
      "outputs": [],
      "source": [
        "#import des outils necessaires\n",
        "from os import listdir\n",
        "from os.path import isdir\n",
        "from PIL import Image\n",
        "from numpy import savez_compressed\n",
        "from numpy import asarray\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "\n",
        "# Ici on reprend la même procedure, puis...\n",
        "# Extraction d'un seul visage avec px 160 en utilisant la fonction extract\n",
        "def extract_face(filename, required_size=(160, 160)):\n",
        "  # charger l'image à partir du fichier\n",
        "  image = Image.open(filename)\n",
        "  # convertir en l'image en RGB\n",
        "  image = image.convert('RGB')\n",
        "  # convertir en tableau\n",
        "  pixels = asarray(image)\n",
        "  # creer notre detecteur de visage mtcnn\n",
        "  detector = MTCNN()\n",
        "  # detection du visage stocké dans results\n",
        "  results = detector.detect_faces(pixels)\n",
        "  # extraire le cadre de limitation de la première face\n",
        "  x1, y1, width, height = results[0]['box']\n",
        "\n",
        "  # corriger les erreurs\n",
        "  x1, y1 = abs(x1), abs(y1)\n",
        "  x2, y2 = x1 + width, y1 + height\n",
        "  # extraire le visage\n",
        "  face = pixels[y1:y2, x1:x2]\n",
        "  # redimension des pixels du modele\n",
        "  image = Image.fromarray(face)\n",
        "  image = image.resize(required_size)\n",
        "  face_array = asarray(image)\n",
        "  return face_array\n",
        "\n",
        "# Utiliser la fonction loadfaces pour étiquetter tous les répertoires des celebrités\n",
        "def load_faces(directory):\n",
        "  faces = list()\n",
        "  # enumerrer les fichiers\n",
        "  for filename in listdir(directory):\n",
        "    # Definir le chemin\n",
        "    path = directory + filename\n",
        "    # Obtenir le visage\n",
        "    face = extract_face(path)\n",
        "    # Tracer\n",
        "    faces.append(face)\n",
        "  return faces\n",
        "\n",
        "# la fonction load_dataset : detection des visages dans les sous repertoires et l'attribution d'étiquettes\n",
        "# charger un ensemble de données qui contient un sous-répertoire pour chaque classe qui à son tour contient des images\n",
        "def load_dataset(directory):\n",
        "  X, y = list(), list()\n",
        "  # # enumerrer les fichiers par classe \n",
        "  for subdir in listdir(directory):\n",
        "    # Definir le chemin\n",
        "    path = directory + subdir + '/'\n",
        "    # Ignorer tous les fichiers qui pourraient être dans le répertoire\n",
        "    if not isdir(path):\n",
        "\t\t    continue\n",
        "    # charge toutes les faces du sous-répertoire\n",
        "    faces = load_faces(path)\n",
        "    # creation des étiquettes\n",
        "    labels = [subdir for _ in range(len(faces))]\n",
        "    # Résumé de la progression\n",
        "    print('> %d exemples chargés pour la classe : %s' % (len(faces), subdir))\n",
        "    # Tracer\n",
        "    X.extend(faces)\n",
        "    y.extend(labels)\n",
        "  return asarray(X), asarray(y)\n",
        "\n",
        "# Charger notre dataset\n",
        "\n",
        "trainX, trainy = load_dataset('data/train/')\n",
        "print(trainX.shape, trainy.shape)\n",
        "# Charger le dataset pour le test\n",
        "testX, testy = load_dataset('/content/data/val')\n",
        "print(testX.shape, testy.shape)\n",
        "# enregistrer les tableaux dans un fichier au format compressé\n",
        "savez_compressed('/content/data', trainX, trainy, testX, testy) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWQe6iECzEUB"
      },
      "source": [
        "Maintenant chargeons notre dataset npz de visages detectés et utilisons facenet pour l'integration de visage. L'objectif, prédire les visages, pour cela, traitons les pixels de images pour facenet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfFuQ0xQj4Om"
      },
      "outputs": [],
      "source": [
        "\n",
        "# calculer une intégration de visage pour chaque visage dans l'ensemble de données à l'aide de facenet\n",
        "# calculate a face embedding for each face in the dataset using facenet\n",
        "from numpy import load\n",
        "from numpy import expand_dims\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "from keras.models import load_model\n",
        "\n",
        "# obtenir l'intégration (signification) du visage pour un visage\n",
        "def get_embedding(model, face_pixels):\n",
        "  # mettre à l'échelle les valeurs des pixels\n",
        "  face_pixels = face_pixels.astype('float32')\n",
        "  # standardiser les valeurs de pixels sur tous les canaux (global)\n",
        "  mean, std = face_pixels.mean(), face_pixels.std()\n",
        "  face_pixels = (face_pixels - mean) / std\n",
        "  # transformer le visage en un seul échantillon\n",
        "  samples = expand_dims(face_pixels, axis=0)\n",
        "  # faire une prédiction pour obtenir l'intégration\n",
        "  yhat = model.predict(samples)\n",
        "  return yhat[0]\n",
        "\n",
        "\n",
        "# Charger les visages dataset\n",
        "data = load('/content/data.npz')\n",
        "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
        "print('Chargé : ', trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        "\n",
        "\n",
        "# Charger le facenet model pour integratin de visage\n",
        "model = load_model('/content/keras-facenet/model/facenet_keras.h5')\n",
        "print('Notre modèle chargé')\n",
        "\n",
        "# Enumerer chaque visage pour faire de l'integration\n",
        "newTrainX = list()\n",
        "for face_pixels in trainX:\n",
        "\tembedding = get_embedding(model, face_pixels)\n",
        "\tnewTrainX.append(embedding)\n",
        "newTrainX = asarray(newTrainX)\n",
        "print(newTrainX.shape)\n",
        "\n",
        "# convertir chaque visage de l'ensemble de test en une intégration\n",
        "newTestX = list()\n",
        "for face_pixels in testX:\n",
        "\tembedding = get_embedding(model, face_pixels)\n",
        "\tnewTestX.append(embedding)\n",
        "newTestX = asarray(newTestX)\n",
        "print(newTestX.shape)\n",
        "\n",
        "# enregistrer les tableaux dans un fichier au format compressé\n",
        "savez_compressed('data_integration.npz', newTrainX, trainy, newTestX, testy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtqrfOxMcIJs"
      },
      "source": [
        "L'ensemble est enregistré sous forme de tableau numpy dans un repertoire compressé."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fShutE_L35Gq"
      },
      "source": [
        "Maintenant classons les inscrustations de visage à l'aide de notre modèle. Utilisation de en utilisant la classe Normalizer dans scikit-learn et la la classe LabelEncoder. Pour normaliser, les vecteurs utilisons SVC de SVM dans scikit-learn. voyons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7Z0fZuKg7nv"
      },
      "source": [
        "La classification des visages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nir_lsqR0L9l"
      },
      "outputs": [],
      "source": [
        "# développer un classificateur pour l'ensemble de données 5 Celebrity Faces et charger les biblio\n",
        "from numpy import load\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "# Charger dataset d'integration de visage\n",
        "data = load('/content/data_integration.npz')\n",
        "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
        "print('Dataset: train=%d, test=%d' % (trainX.shape[0], testX.shape[0]))\n",
        "\n",
        "# normaliser les vecteurs d'insertion de visage\n",
        "in_encoder = Normalizer(norm='l2')\n",
        "trainX = in_encoder.transform(trainX)\n",
        "testX = in_encoder.transform(testX)\n",
        "\n",
        "# encodage d'étiquettes pour les cibles\n",
        "out_encoder = LabelEncoder()\n",
        "out_encoder.fit(trainy)\n",
        "trainy = out_encoder.transform(trainy)\n",
        "testy = out_encoder.transform(testy)\n",
        "\n",
        "# Entrainement et ajustement du modele\n",
        "model = SVC(kernel='linear', probability=True)\n",
        "model.fit(trainX, trainy)\n",
        "\n",
        "\n",
        "# Notre prediction\n",
        "yhat_train = model.predict(trainX)\n",
        "yhat_test = model.predict(testX)\n",
        "\n",
        "# Voir le score\n",
        "score_train = accuracy_score(trainy, yhat_train)\n",
        "score_test = accuracy_score(testy, yhat_test)\n",
        "# Notre summarize\n",
        "print('Précision: train=%.3f, test=%.3f' % (score_train*100, score_test*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqzK8W9wiqIr"
      },
      "source": [
        "Pour plus de précision de notre résultat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zPZShymBiktL"
      },
      "outputs": [],
      "source": [
        "# Notre classificateur pour les celebrités\n",
        "from random import choice\n",
        "from numpy import load\n",
        "from numpy import expand_dims\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.svm import SVC\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# Nous reprenons presque la même méthode\n",
        "\n",
        "# Chargement de nos visages\n",
        "data = load('5-celebrity-faces-dataset.npz')\n",
        "testX_faces = data['arr_2']\n",
        "# visage d'integration chargé\n",
        "data = load('data_integration.npz')\n",
        "trainX, trainy, testX, testy = data['arr_0'], data['arr_1'], data['arr_2'], data['arr_3']\n",
        "\n",
        "# normalisation des vecteurs\n",
        "in_encoder = Normalizer(norm='l2')\n",
        "trainX = in_encoder.transform(trainX)\n",
        "testX = in_encoder.transform(testX)\n",
        "\n",
        "# labelisation des cibles\n",
        "out_encoder = LabelEncoder()\n",
        "out_encoder.fit(trainy)\n",
        "trainy = out_encoder.transform(trainy)\n",
        "testy = out_encoder.transform(testy)\n",
        "\n",
        "# ajustement du modele\n",
        "model = SVC(kernel='linear', probability=True)\n",
        "model.fit(trainX, trainy)\n",
        "\n",
        "# modèle de test sur un exemple aléatoire de l'ensemble de données de test\n",
        "selection = choice([i for i in range(testX.shape[0])])\n",
        "random_face_pixels = testX_faces[selection]\n",
        "random_face_emb = testX[selection]\n",
        "random_face_class = testy[selection]\n",
        "random_face_name = out_encoder.inverse_transform([random_face_class])\n",
        "\n",
        "# prediction du visage \n",
        "samples = expand_dims(random_face_emb, axis=0)\n",
        "yhat_class = model.predict(samples)\n",
        "yhat_prob = model.predict_proba(samples)\n",
        "\n",
        "# Obtenir l'étiquette\n",
        "class_index = yhat_class[0]\n",
        "class_probability = yhat_prob[0,class_index] * 100\n",
        "predict_names = out_encoder.inverse_transform(yhat_class)\n",
        "\n",
        "print('Visage prédit : %s (%.3f)' % (predict_names[0], class_probability))\n",
        "print('Visage attendue: %s' % random_face_name[0])\n",
        "\n",
        "# Traçage\n",
        "pyplot.imshow(random_face_pixels)\n",
        "title = '%s (%.3f)' % (predict_names[0], class_probability)\n",
        "pyplot.title(title)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans ce projet, nous avons utiliser un modèle Facenet et un classificateur SVM pour identifier les visages sur des images.\n",
        "\n",
        "Nous avons préparés les datasets, effectuer une detection de visages en extrayant les caracteristiques de visages et l'integration de visages.\n",
        "Puis, nous avons utiliser SVM pour l'identification dans un système d'integration de visages.\n",
        "\n",
        "Resultat : nom d'etiquette et prédiction."
      ],
      "metadata": {
        "id": "pJjyCeZBjoKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons aller en approfondir le sujet de l'apprentissage en profondeur pour l'ordinateur\n",
        "vision."
      ],
      "metadata": {
        "id": "nsfpmKGzk5KJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "swKhsuWik8tc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "FaceNet Recognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1WSpsdmunQjMfgIsdaFMERE6IdX87j8QH",
      "authorship_tag": "ABX9TyPhqDwbmEPhZaJq0J+X2D4b",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}